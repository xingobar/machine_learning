{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from logistic_regression import LogisticRegressioin,load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Hidden(object):\n",
    "    '''\n",
    "    For tanh activation function results obtained in [Xavier10] show that \n",
    "    the interval should be [-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}], \n",
    "    '''\n",
    "    def __init__(self , random_stream, input, n_in , \n",
    "                 n_out , weight= None,  bias= None,activation=T.tanh):\n",
    "        if weight is None:\n",
    "            W_values = np.asarray(\n",
    "                random_stream.uniform(\n",
    "                    low = -np.sqrt(6./(n_in + n_out)),\n",
    "                    upper = np.sqrt(6./(n_in + n_out))\n",
    "                ),\n",
    "                dtype = theano.config.floatX\n",
    "            )\n",
    "            if activation == tensor.nnet.sigmoid:\n",
    "                W_value *=4\n",
    "            Weight = theano.shared(value=W_values,name='weight',borrow = True) # false is deepcopy\n",
    "\n",
    "        if bias is None:\n",
    "            bias_values = np.zeros(n_out,dtype = theano.config.floatX)\n",
    "            bias = theano.shared(value=bias_values,name='bias',borrow=True)\n",
    "\n",
    "        self.W = Weight\n",
    "        self.b = bias\n",
    "        self.input = input\n",
    "        self.params = [self.W,self.b] # parameter of the model\n",
    "        linear_output = T.dot(input,self.W) + self.b\n",
    "        self.output = [linear_output if activation is None else activation(linear_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self,random_steam,input,n_in,n_hidden,n_out):\n",
    "        \n",
    "        self.hidden_layer = HiddenLayer(random_stream,\n",
    "                                        input = input,\n",
    "                                        n_in = n_in,\n",
    "                                        n_out = n_hidden,\n",
    "                                        activation = T.tanh)\n",
    "        \n",
    "        self.LogisticRegressionLayer = LogisticRegression(\n",
    "                                        input = self.hidden_layer.output,\n",
    "                                        n_in = n_hidden,\n",
    "                                        n_out = n_out,\n",
    "                                        activation = T.tanh\n",
    "                                        ) \n",
    "        \n",
    "        ## compute l1 norm (sum) and squared l2 norm\n",
    "        self.L1 = (\n",
    "                    abs(self.hidden_layer.W) + abs(self.LogisticRegressionLayer.W)\n",
    "                 )\n",
    "        \n",
    "        self.L2 = (\n",
    "                     (self.hidden_layer.W **2).sum() + (self.LogisticRegressionLayer.W **2).sum()\n",
    "                    )\n",
    "        \n",
    "        self.neg_loglikelihood = self.LogisticRegressionLayer.negative_loglikelihood\n",
    "        self.error = self.LogisticRegressionLayer.error\n",
    "        self.params = self.hidden_layer.params + self.LogisticRegressionLayer.params\n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(learning_rate = 0.01,l1_reg=0.0,l2_reg=0.0001,\n",
    "         n_epoch=1000,batch_size=20,hidden_units=500):\n",
    "    \n",
    "    dataset = load_data()\n",
    "    trian_x,train_y = dataset[0]\n",
    "    validation_x,validation_y = dataset[1]\n",
    "    test_x,test_y = dataset[2]\n",
    "    \n",
    "    ## compute the number of minibatches\n",
    "    n_train_batches = train_x.get_value(borrow=True).shape[0] // 2\n",
    "    n_test_batches = test_x.get_value(borrow=True).shape[0] //2\n",
    "    n_validation_batches = validation_x.get_value(borrow=True).shape[0] //2\n",
    "    \n",
    "    print 'building the model....'\n",
    "    index = T.lscalar() ## index\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y') ## labels \n",
    "    random_state = np.random.RandomState(1234)\n",
    "    \n",
    "    classifier = MLP(\n",
    "                    random_state = random_state,\n",
    "                    input = x,\n",
    "                    n_in = 28*28,\n",
    "                    n_hidden = hidden_units,\n",
    "                    n_out = 10\n",
    "                )\n",
    "    \n",
    "    # loss function (cost function) plus regularization \n",
    "    cost  = (\n",
    "            classifier.neg_loglikelihood(y) + \n",
    "            l1_reg * classifier.L1 +\n",
    "            l2_reg * classifier.L2\n",
    "    )\n",
    "    \n",
    "    test_model = theano.function(\n",
    "                inputs =[index],\n",
    "                outputs = classifier.error(y),\n",
    "                givens = {\n",
    "                    x:test_x[index * batch_size : (index+1) * batch_size],\n",
    "                    y:test_y[index * batch_size : (index+1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    validation_model = theano.function(\n",
    "                inputs = [index],\n",
    "                outputs = classifier.error(y),\n",
    "                givens ={\n",
    "                    x:validation_x[index * batch_size : (index+1) * batch_size],\n",
    "                    y:validation_y[index * batch_size : (index+1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    ## gradient descent\n",
    "    gparams = [T.grad(cost,params) for params in classifier.params]\n",
    "    \n",
    "    updates = [(params , params - learning_rate * gparams)  for params,gparams in zip(classifier.params,gparams)]\n",
    "    \n",
    "    train_model = theano.function(\n",
    "                inputs = [index],\n",
    "                outputs = cost,\n",
    "                updates = updates,\n",
    "                givens = {\n",
    "                    x:train_x[index * batch_size : (index+1) * batch_size],\n",
    "                    y:train_y[index * batch_size : (index+1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print 'complete the building model'\n",
    "    print 'training the model....'\n",
    "    \n",
    "    patience = 10000\n",
    "    patience_improvement = 2\n",
    "    improvement_threshold = 0.995\n",
    "    validation_frequency = min(n_train_batches , patience //2)\n",
    "    best_validation_loss = np.inf\n",
    "    best_iteration = 0.\n",
    "    test_score = 0.\n",
    "    start_time = time.time()\n",
    "    epoch = 0\n",
    "    looping = False\n",
    "    \n",
    "    print 'complete the training model'\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
