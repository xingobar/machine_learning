{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print 'Loading data....'\n",
    "    with open('/Users/xingobar/Downloads/mnist.pkl', 'rb') as f:\n",
    "        train_data,validation_data,test_data = pickle.load(f)\n",
    "    \n",
    "    def share_dataset(data,borrow=True):\n",
    "        data_x,data_y = data\n",
    "        shared_x = theano.shared(np.asarray(data_x,dtype=theano.config.floatX),borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y,dtype=theano.config.floatX),borrow=borrow)\n",
    "        return shared_x,shared_y\n",
    "    \n",
    "    train_x,train_y = share_dataset(train_data)\n",
    "    validation_x,validation_y = share_dataset(validation_data)\n",
    "    test_x,test_y = share_dataset(test_data)\n",
    "    flatten = [(train_x,theano.tensor.cast(train_y.flatten(),'int32')),\n",
    "               (validation_x,theano.tensor.cast(validation_y.flatten(),'int32')),\n",
    "               (test_x,theano.tensor.cast(test_y.flatten(),'int32'))]\n",
    "    return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegressioin(object):\n",
    "    def __init__(self,input,n_in,n_out):\n",
    "        self.W = theano.shared(np.zeros((n_in,n_out),dtype=theano.config.floatX),name='weight',borrow=True)\n",
    "        self.b = theano.shared(np.zeros((n_out),dtype=theano.config.floatX),name='bias',borrow=True)\n",
    "        self.y_prob_given_x  =  theano.tensor.nnet.softmax(theano.tensor.dot(input,self.W) + self.b)\n",
    "        self.y_pred = theano.tensor.argmax(self.y_prob_given_x,axis=1) \n",
    "        self.params = [self.W,self.b]\n",
    "        self.input = input\n",
    "    \n",
    "    def negative_log_likelihood(self,y):\n",
    "        return -theano.tensor.mean(theano.tensor.log(self.y_prob_given_x)[theano.tensor.arange(y.shape[0]),y])\n",
    "    \n",
    "    def error(self,y):\n",
    "        if y.dtype.startswith('int'):\n",
    "            return theano.tensor.mean(theano.tensor.neq(self.y_pred,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data....\n",
      "number of training batches :  83\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 600\n",
    "dataset = load_data()\n",
    "train_x,train_y = dataset[0]\n",
    "validation_x,validation_y = dataset[1]\n",
    "n_train_batch = train_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_validation_batch  = validation_x.get_value(borrow=True).shape[0] / batch_size \n",
    "print 'number of training batches : ' , n_train_batch\n",
    "print train_x.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the model....\n",
      "complete the training model\n"
     ]
    }
   ],
   "source": [
    "print 'building the model....'\n",
    "index = theano.tensor.lscalar()\n",
    "learning_rate = 0.01\n",
    "x = theano.tensor.matrix('x') # data\n",
    "y =  theano.tensor.ivector('y') # labels , 1D vector\n",
    "classifier = LogisticRegressioin(input = x , n_in = 28*28,n_out = 10)\n",
    "cost  = classifier.negative_log_likelihood(y)\n",
    "\n",
    "validation_model = theano.function(\n",
    "inputs = [index],\n",
    "outputs = classifier.error(y),\n",
    "givens = {\n",
    "        x:validation_x[index * batch_size : (index+1) * batch_size],\n",
    "        y:validation_y[index * batch_size : (index+1) * batch_size]\n",
    "    }\n",
    ")\n",
    "g_w = theano.tensor.grad(cost=cost,wrt=classifier.W)\n",
    "g_b = theano.tensor.grad(cost=cost,wrt=classifier.b)\n",
    "\n",
    "updates = [(classifier.W, classifier.W - learning_rate * g_w),\n",
    "           (classifier.b,classifier.b - learning_rate * g_b)]\n",
    "\n",
    "train_model = theano.function(\n",
    "inputs = [index],\n",
    "outputs = cost,\n",
    "updates = updates,\n",
    "givens = {\n",
    "        x:train_x[index * batch_size : (index+1) * batch_size],\n",
    "        y:train_y[index * batch_size : (index+1) * batch_size]\n",
    "    }\n",
    ")\n",
    "print 'complete the building model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_optimization(learning_rate = 0.13,n_epoch = 1000, batch_size=600):\n",
    "    dataset = load_data()\n",
    "    train_x,train_y = dataset[0]\n",
    "    validation_x,validation_y = dataset[1]\n",
    "    test_x,test_y = dataset[2]\n",
    "    n_train_batch = train_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_validation_batch  = validation_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_test_batch = test_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    print 'number of training batches : ' , n_train_batch\n",
    "    \n",
    "    \n",
    "    print 'building the model....'\n",
    "    index = theano.tensor.lscalar() # index to a minibatch\n",
    "    #learning_rate = 0.01\n",
    "    x = theano.tensor.matrix('x') # data\n",
    "    y =  theano.tensor.ivector('y') # labels , 1D vector\n",
    "    classifier = LogisticRegressioin(input = x , n_in = 28*28,n_out = 10)\n",
    "    cost  = classifier.negative_log_likelihood(y) # loss function\n",
    "\n",
    "    test_model = theano.function(\n",
    "    inputs =[index],\n",
    "    outputs = classifier.error(y),\n",
    "    givens = {\n",
    "            x:test_x[index * batch_size : (index+1) * batch_size],\n",
    "            y:test_y[index * batch_size : (index+1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    validation_model = theano.function(\n",
    "    inputs = [index],\n",
    "    outputs = classifier.error(y),\n",
    "    givens = {\n",
    "            x:validation_x[index * batch_size : (index+1) * batch_size],\n",
    "            y:validation_y[index * batch_size : (index+1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # gradient descent\n",
    "    g_w = theano.tensor.grad(cost=cost,wrt=classifier.W)\n",
    "    g_b = theano.tensor.grad(cost=cost,wrt=classifier.b)\n",
    "\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_w),\n",
    "               (classifier.b,classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    train_model = theano.function(\n",
    "    inputs = [index],\n",
    "    outputs = cost,\n",
    "    updates = updates,\n",
    "    givens = {\n",
    "            x:train_x[index * batch_size : (index+1) * batch_size],\n",
    "            y:train_y[index * batch_size : (index+1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    print 'complete the building model'\n",
    "    \n",
    "    print 'Start training the model.....'\n",
    "    ## early stopping\n",
    "    patience = 5000\n",
    "    patience_increase = 2\n",
    "    improvement_threshold = 0.995\n",
    "    validation_frequency = min(n_train_batch,patience // 2)\n",
    "    best_validation_loss = np.inf\n",
    "    test_score = 0 \n",
    "    start_time = time.time()\n",
    "    epoch = 0 \n",
    "    looping = False\n",
    "    \n",
    "    while (epoch < n_epoch) and (not looping):\n",
    "        epoch +=1\n",
    "        for minibatch_index in xrange(n_train_batch):\n",
    "            minibatch_cost = train_model(minibatch_index) ## output cost\n",
    "            n_iteration = (epoch - 1  ) * n_train_batch + minibatch_index\n",
    "            if (n_iteration + 1)  % validation_frequency ==0 : ## compute the validation losses per validation\n",
    "                validation_losses = [validation_model(i) for i in xrange(n_validation_batch)] ## output errors\n",
    "                this_validation_losses = np.mean(validation_losses)\n",
    "                print 'epoch %i , minibatch %i/%i , validation error %f ' %(epoch,\n",
    "                                                                            minibatch_index+1,\n",
    "                                                                            n_train_batch,\n",
    "                                                                            this_validation_losses * 100)\n",
    "    \n",
    "                if this_validation_losses < best_validation_loss:\n",
    "                    if this_validation_losses < best_validation_loss * improvement_threshold:\n",
    "                        patience = max(patience , n_iteration * patience_increase)\n",
    "                    best_validation_loss = this_validation_losses\n",
    "                \n",
    "                    test_losses = [test_model(i) for i in xrange(n_test_batch)] ## output errors\n",
    "                    test_score = np.mean(test_losses)\n",
    "                    print 'epoch %i, minibatch %i/%i , test score %f ' %(epoch,\n",
    "                                                                      minibatch_index+1,\n",
    "                                                                      n_train_batch,\n",
    "                                                                      test_score * 100)\n",
    "                    \n",
    "                    ## save the best model \n",
    "                    with open('logistic_regression_best_model.pkl','wb') as f:\n",
    "                        pickle.dump(classifier,f)\n",
    "                        \n",
    "            if patience <= n_iteration:\n",
    "                looping = True\n",
    "                break\n",
    "    end_time = time.time()          \n",
    "    print 'Complete the trainig the model.....'\n",
    "    print 'Time is %0.2f' %((end_time - start_time) / 60)\n",
    "    print 'best validation score of %f with test score %f' % (best_validation_loss * 100, test_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    classifier = pickle.load(open('logistic_regression_best_model.pkl','rb'))\n",
    "    predicited_model = theano.function(\n",
    "    inputs =[classifier.input],\n",
    "    outputs = classifier.y_pred\n",
    "    )\n",
    "    \n",
    "    dataset = load_data()\n",
    "    test_x,test_y = dataset[2]\n",
    "    test_x = test_x.get_value()\n",
    "    y_pred = predicited_model(test_x[:10])\n",
    "    print y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data....\n",
      "number of training batches :  83\n",
      "building the model....\n",
      "complete the building model\n",
      "Start training the model.....\n",
      "epoch 1 , minibatch 83/83 , validation error 12.458333 \n",
      "epoch 1, minibatch 83/83 , test score 12.375000 \n",
      "epoch 2 , minibatch 83/83 , validation error 11.010417 \n",
      "epoch 2, minibatch 83/83 , test score 10.958333 \n",
      "epoch 3 , minibatch 83/83 , validation error 10.312500 \n",
      "epoch 3, minibatch 83/83 , test score 10.312500 \n",
      "epoch 4 , minibatch 83/83 , validation error 9.875000 \n",
      "epoch 4, minibatch 83/83 , test score 9.833333 \n",
      "epoch 5 , minibatch 83/83 , validation error 9.562500 \n",
      "epoch 5, minibatch 83/83 , test score 9.479167 \n",
      "epoch 6 , minibatch 83/83 , validation error 9.322917 \n",
      "epoch 6, minibatch 83/83 , test score 9.291667 \n",
      "epoch 7 , minibatch 83/83 , validation error 9.187500 \n",
      "epoch 7, minibatch 83/83 , test score 9.000000 \n",
      "epoch 8 , minibatch 83/83 , validation error 8.989583 \n",
      "epoch 8, minibatch 83/83 , test score 8.958333 \n",
      "epoch 9 , minibatch 83/83 , validation error 8.937500 \n",
      "epoch 9, minibatch 83/83 , test score 8.812500 \n",
      "epoch 10 , minibatch 83/83 , validation error 8.750000 \n",
      "epoch 10, minibatch 83/83 , test score 8.666667 \n",
      "epoch 11 , minibatch 83/83 , validation error 8.666667 \n",
      "epoch 11, minibatch 83/83 , test score 8.520833 \n",
      "epoch 12 , minibatch 83/83 , validation error 8.583333 \n",
      "epoch 12, minibatch 83/83 , test score 8.416667 \n",
      "epoch 13 , minibatch 83/83 , validation error 8.489583 \n",
      "epoch 13, minibatch 83/83 , test score 8.291667 \n",
      "epoch 14 , minibatch 83/83 , validation error 8.427083 \n",
      "epoch 14, minibatch 83/83 , test score 8.281250 \n",
      "epoch 15 , minibatch 83/83 , validation error 8.354167 \n",
      "epoch 15, minibatch 83/83 , test score 8.270833 \n",
      "epoch 16 , minibatch 83/83 , validation error 8.302083 \n",
      "epoch 16, minibatch 83/83 , test score 8.239583 \n",
      "epoch 17 , minibatch 83/83 , validation error 8.250000 \n",
      "epoch 17, minibatch 83/83 , test score 8.177083 \n",
      "epoch 18 , minibatch 83/83 , validation error 8.229167 \n",
      "epoch 18, minibatch 83/83 , test score 8.062500 \n",
      "epoch 19 , minibatch 83/83 , validation error 8.260417 \n",
      "epoch 20 , minibatch 83/83 , validation error 8.260417 \n",
      "epoch 21 , minibatch 83/83 , validation error 8.208333 \n",
      "epoch 21, minibatch 83/83 , test score 7.947917 \n",
      "epoch 22 , minibatch 83/83 , validation error 8.187500 \n",
      "epoch 22, minibatch 83/83 , test score 7.927083 \n",
      "epoch 23 , minibatch 83/83 , validation error 8.156250 \n",
      "epoch 23, minibatch 83/83 , test score 7.958333 \n",
      "epoch 24 , minibatch 83/83 , validation error 8.114583 \n",
      "epoch 24, minibatch 83/83 , test score 7.947917 \n",
      "epoch 25 , minibatch 83/83 , validation error 8.093750 \n",
      "epoch 25, minibatch 83/83 , test score 7.947917 \n",
      "epoch 26 , minibatch 83/83 , validation error 8.104167 \n",
      "epoch 27 , minibatch 83/83 , validation error 8.104167 \n",
      "epoch 28 , minibatch 83/83 , validation error 8.052083 \n",
      "epoch 28, minibatch 83/83 , test score 7.843750 \n",
      "epoch 29 , minibatch 83/83 , validation error 8.052083 \n",
      "epoch 30 , minibatch 83/83 , validation error 8.031250 \n",
      "epoch 30, minibatch 83/83 , test score 7.843750 \n",
      "epoch 31 , minibatch 83/83 , validation error 8.010417 \n",
      "epoch 31, minibatch 83/83 , test score 7.833333 \n",
      "epoch 32 , minibatch 83/83 , validation error 7.979167 \n",
      "epoch 32, minibatch 83/83 , test score 7.812500 \n",
      "epoch 33 , minibatch 83/83 , validation error 7.947917 \n",
      "epoch 33, minibatch 83/83 , test score 7.739583 \n",
      "epoch 34 , minibatch 83/83 , validation error 7.875000 \n",
      "epoch 34, minibatch 83/83 , test score 7.729167 \n",
      "epoch 35 , minibatch 83/83 , validation error 7.885417 \n",
      "epoch 36 , minibatch 83/83 , validation error 7.843750 \n",
      "epoch 36, minibatch 83/83 , test score 7.697917 \n",
      "epoch 37 , minibatch 83/83 , validation error 7.802083 \n",
      "epoch 37, minibatch 83/83 , test score 7.635417 \n",
      "epoch 38 , minibatch 83/83 , validation error 7.812500 \n",
      "epoch 39 , minibatch 83/83 , validation error 7.812500 \n",
      "epoch 40 , minibatch 83/83 , validation error 7.822917 \n",
      "epoch 41 , minibatch 83/83 , validation error 7.791667 \n",
      "epoch 41, minibatch 83/83 , test score 7.625000 \n",
      "epoch 42 , minibatch 83/83 , validation error 7.770833 \n",
      "epoch 42, minibatch 83/83 , test score 7.614583 \n",
      "epoch 43 , minibatch 83/83 , validation error 7.750000 \n",
      "epoch 43, minibatch 83/83 , test score 7.593750 \n",
      "epoch 44 , minibatch 83/83 , validation error 7.739583 \n",
      "epoch 44, minibatch 83/83 , test score 7.593750 \n",
      "epoch 45 , minibatch 83/83 , validation error 7.739583 \n",
      "epoch 46 , minibatch 83/83 , validation error 7.739583 \n",
      "epoch 47 , minibatch 83/83 , validation error 7.739583 \n",
      "epoch 48 , minibatch 83/83 , validation error 7.708333 \n",
      "epoch 48, minibatch 83/83 , test score 7.583333 \n",
      "epoch 49 , minibatch 83/83 , validation error 7.677083 \n",
      "epoch 49, minibatch 83/83 , test score 7.572917 \n",
      "epoch 50 , minibatch 83/83 , validation error 7.677083 \n",
      "epoch 51 , minibatch 83/83 , validation error 7.677083 \n",
      "epoch 52 , minibatch 83/83 , validation error 7.656250 \n",
      "epoch 52, minibatch 83/83 , test score 7.541667 \n",
      "epoch 53 , minibatch 83/83 , validation error 7.656250 \n",
      "epoch 54 , minibatch 83/83 , validation error 7.635417 \n",
      "epoch 54, minibatch 83/83 , test score 7.520833 \n",
      "epoch 55 , minibatch 83/83 , validation error 7.635417 \n",
      "epoch 56 , minibatch 83/83 , validation error 7.635417 \n",
      "epoch 57 , minibatch 83/83 , validation error 7.604167 \n",
      "epoch 57, minibatch 83/83 , test score 7.489583 \n",
      "epoch 58 , minibatch 83/83 , validation error 7.583333 \n",
      "epoch 58, minibatch 83/83 , test score 7.458333 \n",
      "epoch 59 , minibatch 83/83 , validation error 7.572917 \n",
      "epoch 59, minibatch 83/83 , test score 7.468750 \n",
      "epoch 60 , minibatch 83/83 , validation error 7.572917 \n",
      "epoch 61 , minibatch 83/83 , validation error 7.583333 \n",
      "epoch 62 , minibatch 83/83 , validation error 7.572917 \n",
      "epoch 62, minibatch 83/83 , test score 7.520833 \n",
      "epoch 63 , minibatch 83/83 , validation error 7.562500 \n",
      "epoch 63, minibatch 83/83 , test score 7.510417 \n",
      "epoch 64 , minibatch 83/83 , validation error 7.572917 \n",
      "epoch 65 , minibatch 83/83 , validation error 7.562500 \n",
      "epoch 66 , minibatch 83/83 , validation error 7.552083 \n",
      "epoch 66, minibatch 83/83 , test score 7.520833 \n",
      "epoch 67 , minibatch 83/83 , validation error 7.552083 \n",
      "epoch 68 , minibatch 83/83 , validation error 7.531250 \n",
      "epoch 68, minibatch 83/83 , test score 7.520833 \n",
      "epoch 69 , minibatch 83/83 , validation error 7.531250 \n",
      "epoch 70 , minibatch 83/83 , validation error 7.510417 \n",
      "epoch 70, minibatch 83/83 , test score 7.500000 \n",
      "epoch 71 , minibatch 83/83 , validation error 7.520833 \n",
      "epoch 72 , minibatch 83/83 , validation error 7.510417 \n",
      "epoch 73 , minibatch 83/83 , validation error 7.500000 \n",
      "epoch 73, minibatch 83/83 , test score 7.489583 \n",
      "Complete the trainig the model.....\n",
      "Time is 0.28\n",
      "best validation score of 7.500000 with test score 7.489583\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    sgd_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data....\n",
      "[7 2 1 0 4 1 4 9 6 9]\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
